@inproceedings{viegas-etal-2020-cluhtm,
    title = "{C}lu{HTM} - Semantic Hierarchical Topic Modeling based on {C}lu{W}ords",
    author = "Viegas, Felipe  and
      Cunha, Washington  and
      Gomes, Christian  and
      Pereira, Ant{\^o}nio  and
      Rocha, Leonardo  and
      Goncalves, Marcos",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.724",
    doi = "10.18653/v1/2020.acl-main.724",
    pages = "8138--8150",
    abstract = "Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the {``}ideal{''} number of topics and depth of the hierarchy. In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM. CluHTM{'}s novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information {--} when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the {``}shape{''} the hierarchical structure. In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500{\%} over the strongest state-of-the-art baselines. We also provide qualitative and quantitative statistical analyses of why our solution works so well.",
}