---
title: Washington Cunha
role: PhD. Student
avatar_filename: avatar.png
bio: My research interests include xxx, xxx and xxx.
interests:
  - Machine Learning
  - Artificial Intelligence
  - Information Retrieval
social:
  - icon: envelope
    icon_pack: fas
    link: emailto:washingtoncunha@dcc.ufmg.br
  - icon: graduation-cap
    icon_pack: fas
    link: https://scholar.google.com.br/citations?user=TiRmr48AAAAJ
  - icon: researchgate
    icon_pack: ai
    link: https://www.researchgate.net/profile/Washington-Cunha
  - icon: github
    icon_pack: fab
    link: https://github.com/waashk
  - icon: linkedin
    icon_pack: fab
    link: https://www.linkedin.com/in/washington-l-m-cunha/
organizations:
  - name: Federal University of Minas Gerais
    url: https://ppgcc.dcc.ufmg.br/
education:
  courses:
    - course: PhD Student in Computer Science (Since 2019)
      institution: Federal University of Minas Gerais
      year: ""
    - course: MSc in Computer Science
      institution: Federal University of Minas Gerais
      year: 2019
    - course: BSc in Computer Science
      institution: Federal University of São João Del-Rei
      year: 2017
email: ""
superuser: true
highlight_name: true
---
My name is Washington Cunha, and I am a PhD Candidate in the Machine Learning and Databases Laboratory (LBD), advised by professor [Marcos Goncalves](https://scholar.google.com.br/citations?user=IStCGaoAAAAJ)  and professor [Leonardo Rocha](https://scholar.google.com.br/citations?user=P3m8CaIAAAAJ) at the Federal University of Minas Gerais. My main research goal focuses on an under-investigated data engineering technique, but whose potential is enormous in the current scenario known as Instance Selection (IS). The IS goal is to reduce the training set size by removing noisy or redundant instances while maintaining the effectiveness of the trained models and reducing the training process cost. In this context, we provided (Accepted on CSUR'23) a comprehensive and scientifically sound comparison of IS methods applied to Text Classification, considering several classification solutions and many datasets, answering questions that reveal an enormous unfulfilled potential for IS solutions. We also proposed (Accepted on SIGIR'23) a two-step framework aimed at large datasets with a particular focus on Transformer architectures. Our solution managed to reduce the training sets by almost 30% on average while maintaining the same levels of effectiveness in all datasets, with speedup improvements of 37% (up to 70%), scaling for datasets with hundreds of thousands of documents.


{{< icon name="download" pack="fas" >}} Download my {{< staticref "uploads/cvwashington.pdf" "newtab" >}}resumé{{< /staticref >}}.
